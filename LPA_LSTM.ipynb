{
 "cells": [
  {
   "cell_type": "code",
   "id": "8d55fffd-83a0-479f-9778-1a65acb8c248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:23.281987Z",
     "start_time": "2026-01-27T12:40:20.092264Z"
    }
   },
   "source": "\"\"\"\nLPA-LSTM: Adaptive Window Estimation for LSTM\n\nThis notebook implements the Local Parametric Approach (LPA) for LSTM-based \nchange point detection, following Spokoiny (1998) and Cizek et al. (2009).\n\nPipeline:\n1. Compute raw critical values via Monte Carlo simulation (residual bootstrap)\n2. Apply Spokoiny-style sqrt(log) adjustment to control false positives\n3. Run LPA detection using adjusted critical values\n4. Save all results (raw CVs, adjusted CVs, detection results)\n\nThe adjustment ensures smaller windows have higher critical values to control\nfalse positive rates across scales (Cizek 2009 eq. 3.8: z_k/n_k = C + D*log(n_k) with D < 0).\n\nOutput structure:\n    LPA/Geometric/Jump_{jump}_N0_{n_0}/lambda{penalty}/\n        - critical_values_raw.csv\n        - critical_values_adjusted.csv\n        - detection_results.csv\n\"\"\"\n\nimport os\nimport math\nimport time\nfrom typing import Dict, Optional\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7a8182b6-1bc4-4575-b265-dddc1abc529d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:23.298812Z",
     "start_time": "2026-01-27T12:40:23.290409Z"
    }
   },
   "source": "# =============================================================================\n# Configuration\n# =============================================================================\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# LSTM hyperparameters\nLSTM_SEQ_LEN = 3\nLSTM_HIDDEN = 16\nLSTM_LAYERS = 1\nLSTM_EPOCHS = 15\nLSTM_BATCH = 64\nLSTM_LR = 1e-2\nLSTM_DROPOUT = 0.0\nMIN_SEG = 20  # Minimum segment size for splits\n\n\n# =============================================================================\n# LSTM Model\n# =============================================================================\n\nclass LSTMRegressor(nn.Module):\n    \"\"\"Simple LSTM for time series regression.\"\"\"\n    \n    def __init__(self, input_size: int = 1, hidden: int = 64,\n                 layers: int = 1, dropout: float = 0.0):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden, num_layers=layers,\n                            batch_first=True, dropout=dropout if layers > 1 else 0.0)\n        self.fc = nn.Linear(hidden, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :]).squeeze(-1)\n\n\n# =============================================================================\n# Sequence Building and LSTM Training\n# =============================================================================\n\ndef build_sequences(y: np.ndarray, seq_len: int, start_abs_idx: int = 0):\n    \"\"\"\n    Build sequences for LSTM from a time series window.\n    \n    Args:\n        y: Time series array (1D float32)\n        seq_len: LSTM sequence length\n        start_abs_idx: Absolute index of first element in y\n        \n    Returns:\n        X: Input sequences [N, seq_len, 1]\n        y_target: Target values [N]\n        t_abs: Absolute target indices [N]\n    \"\"\"\n    n = len(y)\n    if n <= seq_len:\n        return None, None, None\n    \n    X = np.lib.stride_tricks.sliding_window_view(y, seq_len + 1)\n    X_seq, y_target = X[:, :-1], X[:, -1]\n    t_abs = np.arange(start_abs_idx + seq_len, start_abs_idx + n, dtype=np.int64)\n    \n    return X_seq[..., None].astype(np.float32), y_target.astype(np.float32), t_abs\n\n\ndef fit_lstm(X: np.ndarray, y: np.ndarray, epochs: int = LSTM_EPOCHS,\n             batch_size: int = LSTM_BATCH, lr: float = LSTM_LR,\n             hidden: int = LSTM_HIDDEN, layers: int = LSTM_LAYERS,\n             dropout: float = LSTM_DROPOUT) -> nn.Module:\n    \"\"\"Fit LSTM and return trained model.\"\"\"\n    if X is None or len(y) == 0:\n        return None\n\n    if DEVICE.type == \"cuda\":\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cudnn.benchmark = True\n\n    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n    dl = DataLoader(ds, batch_size=batch_size, shuffle=True,\n                    pin_memory=(DEVICE.type == \"cuda\"), num_workers=0)\n\n    model = LSTMRegressor(hidden=hidden, layers=layers, dropout=dropout).to(DEVICE)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n\n    for _ in range(epochs):\n        model.train()\n        for xb, yb in dl:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            opt.zero_grad()\n            loss_fn(model(xb), yb).backward()\n            opt.step()\n\n    return model\n\n\ndef predict_lstm(model: nn.Module, X: np.ndarray) -> np.ndarray:\n    \"\"\"Get predictions from trained model.\"\"\"\n    if model is None or X is None:\n        return None\n\n    model.eval()\n    ds = TensorDataset(torch.from_numpy(X))\n    dl = DataLoader(ds, batch_size=512, shuffle=False)\n\n    preds = []\n    with torch.no_grad():\n        for (xb,) in dl:\n            preds.append(model(xb.to(DEVICE)).cpu().numpy())\n\n    return np.concatenate(preds).astype(np.float32)\n\n\ndef get_residuals(model: nn.Module, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"Get residuals from fitted model.\"\"\"\n    yhat = predict_lstm(model, X)\n    if yhat is None:\n        return None\n    return (y - yhat).astype(np.float32)\n\n\ndef fit_lstm_sse(X: np.ndarray, y: np.ndarray, **kwargs) -> tuple:\n    \"\"\"\n    Fit LSTM and return (SSE, m, yhat, resid).\n    \n    Returns:\n        SSE: Sum of squared errors\n        m: Number of samples\n        yhat: Predictions\n        resid: Residuals\n    \"\"\"\n    if X is None or len(y) == 0:\n        return math.inf, 0, None, None\n\n    model = fit_lstm(X, y, **kwargs)\n    yhat = predict_lstm(model, X)\n    \n    if yhat is None:\n        return math.inf, 0, None, None\n    \n    resid = y - yhat\n    SSE = float(np.sum(resid**2))\n    m = len(y)\n    \n    return SSE, m, yhat.astype(np.float32), resid.astype(np.float32)\n\n\n# =============================================================================\n# Likelihood Computation\n# =============================================================================\n\ndef log_likelihood(SSE: float, m: int) -> float:\n    \"\"\"Gaussian log-likelihood (used for LR statistic).\"\"\"\n    if SSE <= 0 or m <= 0:\n        return -math.inf\n    return -(m / 2) * np.log(SSE)\n\n\n# =============================================================================\n# Wild Bootstrap for Monte Carlo Simulation\n# =============================================================================\n\ndef draw_mammen(m: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Draw Mammen two-point distribution weights for wild bootstrap.\n    E[w] = 0, E[w^2] = 1, E[w^3] = 1\n    \"\"\"\n    p = (np.sqrt(5) + 1) / (2 * np.sqrt(5))\n    a = (1 - np.sqrt(5)) / 2   # approx -0.618\n    b = (1 + np.sqrt(5)) / 2   # approx  1.618\n    u = rng.random(m)\n    return np.where(u < p, a, b).astype(np.float32)\n\n\ndef fit_i0_model(data: np.ndarray, n_0: int, seq_len: int = LSTM_SEQ_LEN,\n                 epochs: int = LSTM_EPOCHS) -> tuple:\n    \"\"\"\n    Fit LSTM on I_0 (last n_0 points) and return model, residuals, seed values.\n    \n    Args:\n        data: Full time series\n        n_0: Initial window size (I_0 size)\n        seq_len: LSTM sequence length\n        epochs: Training epochs\n        \n    Returns:\n        (model, residuals, seed_values)\n    \"\"\"\n    i0_data = data[-n_0:].astype(np.float32)\n    X, y, _ = build_sequences(i0_data, seq_len, 0)\n    \n    if X is None:\n        raise ValueError(f\"I_0 too short: need > {seq_len} points, got {n_0}\")\n\n    model = fit_lstm(X, y, epochs=epochs)\n    residuals = get_residuals(model, X, y)\n    seed_values = i0_data[-seq_len:]\n\n    return model, residuals, seed_values\n\n\ndef simulate_series_from_model(model: nn.Module, seed_values: np.ndarray,\n                                residual_pool: np.ndarray, n_total: int,\n                                seq_len: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Simulate series via recursive 1-step forecasts + wild bootstrap residuals.\n    \n    Args:\n        model: Fitted LSTM model\n        seed_values: Initial values (at least seq_len points)\n        residual_pool: Pool of residuals to resample from\n        n_total: Total length to generate\n        seq_len: LSTM sequence length\n        rng: Random generator\n        \n    Returns:\n        Simulated series of length n_total\n    \"\"\"\n    if n_total <= 0:\n        return np.array([], dtype=np.float32)\n    if model is None or residual_pool is None or len(residual_pool) == 0:\n        raise ValueError(\"Model or residual pool missing\")\n    if len(seed_values) < seq_len:\n        raise ValueError(\"Not enough seed values\")\n\n    y_star = np.empty(n_total, dtype=np.float32)\n\n    # Initialize with seed\n    init = np.asarray(seed_values[-seq_len:], dtype=np.float32)\n    n_init = min(seq_len, n_total)\n    y_star[:n_init] = init[:n_init]\n\n    # Recursive simulation\n    model.eval()\n    with torch.no_grad():\n        for t in range(seq_len, n_total):\n            x = y_star[t - seq_len:t].reshape(1, seq_len, 1)\n            xb = torch.from_numpy(x).to(DEVICE)\n            pred = float(model(xb).item())\n\n            # Wild bootstrap residual\n            e = float(residual_pool[rng.integers(0, len(residual_pool))])\n            w = float(draw_mammen(1, rng)[0])\n            y_star[t] = np.float32(pred + w * e)\n\n    return y_star",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "91bd339b-97e4-4de4-b8c6-4b9a027dccd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:23.309798Z",
     "start_time": "2026-01-27T12:40:23.303026Z"
    }
   },
   "source": "# =============================================================================\n# Critical Value Computation via Monte Carlo\n# =============================================================================\n\ndef compute_suplr_for_series(y: np.ndarray, seq_len: int, search_step: int,\n                              j_start_pos: int, j_end_pos: int,\n                              epochs: int = LSTM_EPOCHS) -> float:\n    \"\"\"\n    Compute SupLR for a single series over specified J_k range.\n    \n    Args:\n        y: Time series array\n        seq_len: LSTM sequence length\n        search_step: Split search granularity\n        j_start_pos: Start of J_k range (series coordinates)\n        j_end_pos: End of J_k range (series coordinates)\n        epochs: LSTM training epochs\n        \n    Returns:\n        Maximum LR statistic over splits in J_k\n    \"\"\"\n    X, y_target, t_idx = build_sequences(y, seq_len, 0)\n    if X is None:\n        return 0.0\n\n    # Fit on full window\n    SSE_full, m_full, _, _ = fit_lstm_sse(X, y_target, epochs=epochs)\n    LL_full = log_likelihood(SSE_full, m_full)\n\n    n_targets = len(y_target)\n\n    # Convert J_k range to target indices\n    split_start = max(MIN_SEG, j_start_pos - seq_len)\n    split_end = min(n_targets - MIN_SEG, j_end_pos - seq_len)\n\n    if split_end <= split_start:\n        return 0.0\n\n    T_vals = []\n    for split_idx in range(split_start, split_end, search_step):\n        X_left, y_left = X[:split_idx], y_target[:split_idx]\n        X_right, y_right = X[split_idx:], y_target[split_idx:]\n\n        if len(y_left) < MIN_SEG or len(y_right) < MIN_SEG:\n            continue\n\n        SSE_left, m_left, _, _ = fit_lstm_sse(X_left, y_left, epochs=epochs)\n        SSE_right, m_right, _, _ = fit_lstm_sse(X_right, y_right, epochs=epochs)\n\n        T_i = (log_likelihood(SSE_left, m_left) +\n               log_likelihood(SSE_right, m_right) - LL_full)\n        T_vals.append(max(0.0, T_i))\n\n    return max(T_vals) if T_vals else 0.0\n\n\ndef compute_critical_values(\n    data: np.ndarray,\n    n_0: int,\n    c: float,\n    mc_reps: int = 300,\n    alpha: float = 0.95,\n    search_step: int = 1,\n    max_len: int = 1500,\n    seq_len: int = LSTM_SEQ_LEN,\n    epochs: int = LSTM_EPOCHS,\n    seed: int = 42,\n    verbose: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Compute raw critical values via Monte Carlo simulation.\n    \n    For each scale k:\n    1. Simulate homogeneous series of length n_{k+1} from I_0 model\n    2. Compute SupLR over J_k split range\n    3. Critical value = alpha-quantile of SupLR distribution\n    \n    Args:\n        data: Time series data\n        n_0: Initial window size\n        c: Geometric ratio for window sizes\n        mc_reps: Number of Monte Carlo replications\n        alpha: Significance level (e.g., 0.95 for 95th percentile)\n        search_step: Split search granularity\n        max_len: Maximum series length to simulate\n        seq_len: LSTM sequence length\n        epochs: LSTM training epochs\n        seed: Random seed\n        verbose: Print progress\n        \n    Returns:\n        DataFrame with columns: k, n_k, n_k_plus1, j_start, j_end, critical_value_95, mean, std\n    \"\"\"\n    try:\n        from tqdm.auto import tqdm\n        has_tqdm = True\n    except ImportError:\n        has_tqdm = False\n        \n    rng = np.random.default_rng(seed)\n    data = np.asarray(data, dtype=np.float32)\n\n    # Step 1: Fit I_0 model\n    if verbose:\n        print(\"Step 1: Fitting LSTM on I_0...\")\n    i0_model, i0_residuals, seed_values = fit_i0_model(data, n_0, seq_len, epochs)\n    if verbose:\n        print(f\"  I_0 size: {n_0}\")\n        print(f\"  Residuals: {len(i0_residuals)} points, std={np.std(i0_residuals):.4f}\")\n\n    # Step 2: Determine scales\n    K_max = 0\n    for k in range(1, 100):\n        n_k_plus1 = int(n_0 * c**(k+1))\n        if n_k_plus1 > max_len:\n            break\n        K_max = k\n\n    if verbose:\n        print(f\"\\nStep 2: Computing critical values for k=1..{K_max}\")\n        print(f\"  MC replications per scale: {mc_reps}\")\n        print(f\"  Device: {DEVICE}\")\n\n    # Step 3: MC simulation for each scale\n    results = []\n    \n    for k in range(1, K_max + 1):\n        n_k_minus1 = n_0 if k == 1 else int(n_0 * c**(k-1))\n        n_k = int(n_0 * c**k)\n        n_k_plus1 = int(n_0 * c**(k+1))\n\n        # J_k range\n        j_start = n_k_plus1 - n_k\n        j_end = n_k_plus1 - n_k_minus1\n\n        suplr_values = []\n        \n        rep_iter = range(mc_reps)\n        if has_tqdm and verbose:\n            rep_iter = tqdm(rep_iter, desc=f\"k={k} (n={n_k_plus1})\", unit=\"rep\", leave=False)\n\n        for _ in rep_iter:\n            # Simulate homogeneous series\n            y_sim = simulate_series_from_model(\n                model=i0_model,\n                seed_values=seed_values,\n                residual_pool=i0_residuals,\n                n_total=n_k_plus1,\n                seq_len=seq_len,\n                rng=rng\n            )\n\n            # Compute SupLR over J_k\n            suplr = compute_suplr_for_series(\n                y_sim, seq_len, search_step, j_start, j_end, epochs\n            )\n            suplr_values.append(suplr)\n\n        suplr_arr = np.array(suplr_values)\n        alpha_pct = int(alpha * 100)\n        \n        results.append({\n            'k': k,\n            'n_k': n_k,\n            'n_k_plus1': n_k_plus1,\n            'j_start': j_start,\n            'j_end': j_end,\n            f'critical_value_{alpha_pct}': float(np.quantile(suplr_arr, alpha)),\n            'critical_value_99': float(np.quantile(suplr_arr, 0.99)),\n            'mean': float(np.mean(suplr_arr)),\n            'std': float(np.std(suplr_arr)),\n        })\n\n        if verbose and not has_tqdm:\n            cv = results[-1][f'critical_value_{alpha_pct}']\n            print(f\"  k={k}: n_k={n_k}, CV({alpha_pct}%)={cv:.3f}\")\n\n    return pd.DataFrame(results)\n\n\n# =============================================================================\n# Spokoiny-Style Critical Value Adjustment\n# =============================================================================\n\ndef adjust_critical_values(\n    df_raw: pd.DataFrame,\n    alpha: float = 0.95,\n    penalty_factor: float = 0.25\n) -> pd.DataFrame:\n    \"\"\"\n    Apply Spokoiny-style adjustment to raw critical values.\n    \n    Following Cizek 2009, the critical value should satisfy:\n        z_k / n_k = C + D * log(n_k)  with D < 0\n    \n    Raw MC critical values typically have D ~ 0 (linear scaling).\n    This adjustment inflates small-window critical values:\n        adjusted_cv = raw_cv * (1 + lambda * sqrt(log(n_K_max / n_k)))\n    \n    Args:\n        df_raw: DataFrame with raw critical values (from compute_critical_values)\n        alpha: Significance level\n        penalty_factor: Lambda parameter (higher = more conservative for small windows)\n        \n    Returns:\n        DataFrame with adjusted critical values\n    \"\"\"\n    df = df_raw.copy()\n    alpha_pct = int(alpha * 100)\n    cv_col = f'critical_value_{alpha_pct}'\n    \n    n_k = df['n_k'].values\n    n_K_max = n_k.max()\n    raw_cv = df[cv_col].values\n\n    # Spokoiny-style adjustment: inflate small windows more\n    ratio = n_K_max / n_k  # >= 1, largest for k=1\n    adjustment = 1 + penalty_factor * np.sqrt(np.log(ratio))\n\n    df['adjustment_factor'] = adjustment\n    df[cv_col] = raw_cv * adjustment\n    \n    # Also adjust 99% if present\n    if 'critical_value_99' in df.columns:\n        df['critical_value_99'] = df['critical_value_99'] * adjustment\n\n    df['penalty_factor'] = penalty_factor\n    \n    return df\n\n\ndef critical_values_to_dict(df: pd.DataFrame, alpha: float = 0.95) -> Dict[int, float]:\n    \"\"\"Convert critical values DataFrame to dict mapping k -> CV.\"\"\"\n    alpha_pct = int(alpha * 100)\n    cv_col = f'critical_value_{alpha_pct}'\n    return {int(row['k']): float(row[cv_col]) for _, row in df.iterrows()}",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "153ec4ad-a277-43ad-960a-0e20bf18174e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:23.317507Z",
     "start_time": "2026-01-27T12:40:23.312991Z"
    }
   },
   "source": "# =============================================================================\n# LPA Change Point Detection\n# =============================================================================\n\ndef detect_changes_with_lstm(\n    Data_N: np.ndarray,\n    critical_values: Dict[int, float],\n    seq_len: int = LSTM_SEQ_LEN,\n    n_0: int = 100,\n    jump: int = 10,\n    search_step: int = 1,\n    c: float = 1.4142,  # sqrt(2), geometric ratio\n    epochs: int = LSTM_EPOCHS,\n    verbose: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Detect change points using LPA with LSTM.\n    \n    Implements the Local Parametric Approach (Spokoiny 1998, Cizek 2009):\n    - At each time point, tests progressively larger windows I_k\n    - Computes SupLR statistic over split candidates in J_k range\n    - Stops when SupLR > critical_value[k] (change detected)\n    - Uses geometric window schedule: n_k = n_0 * c^k\n    \n    Args:\n        Data_N: Time series array (1D)\n        critical_values: Dict mapping scale k -> critical value\n        seq_len: LSTM sequence length\n        n_0: Initial window size\n        jump: Step size for moving through time series\n        search_step: Step size for split search within J_k\n        c: Geometric ratio for window sizes\n        epochs: LSTM training epochs\n        verbose: Print progress\n        \n    Returns:\n        DataFrame with columns: Date, N, windows_1, scaled_windows_1\n    \"\"\"\n    Data_N = np.asarray(Data_N, dtype=np.float32)\n    T = len(Data_N)\n    \n    results = {\n        \"Date\": np.arange(T),\n        \"N\": Data_N,\n        \"windows_1\": [np.nan] * T,\n        \"scaled_windows_1\": [np.nan] * T,\n    }\n    \n    # Process from end to beginning\n    step_indices = list(range(0, T, jump))\n    \n    for l in step_indices:\n        t0 = time.time()\n        io = T - l  # Current time index (working backwards)\n        \n        if io <= n_0:\n            continue\n        \n        # Maximum scale K for this time point\n        K_max = max(0, math.ceil((math.log(io) - math.log(n_0)) / math.log(c)))\n        \n        # Initialize with I_0\n        I_k_minus1 = Data_N[max(0, io - n_0):io]\n        n_k_minus1 = n_0\n        selected_window_size = n_0\n        \n        for k in range(1, K_max + 1):\n            # Geometric window sizes\n            n_k = int(n_0 * c**k)\n            n_k_plus1 = int(n_0 * c**(k + 1))\n            \n            if n_k_plus1 > io:\n                break\n            \n            # Extract window I_{k+1}\n            start_abs = max(0, io - n_k_plus1)\n            y_win = Data_N[start_abs:io].astype(np.float32)\n            \n            # Build sequences\n            X_all, y_all, t_abs = build_sequences(y_win, seq_len, start_abs)\n            if X_all is None:\n                continue\n            \n            # Fit on full window I_{k+1}\n            SSE_full, m_full, _, _ = fit_lstm_sse(X_all, y_all, epochs=epochs)\n            LL_full = log_likelihood(SSE_full, m_full)\n            \n            # J_k split range\n            J_start = max(seq_len, io - n_k)\n            J_end = io - n_k_minus1\n            \n            if J_end <= J_start:\n                continue\n            \n            J_abs = np.arange(J_start, J_end, search_step, dtype=np.int64)\n            \n            # Compute SupLR over J_k\n            T_vals = []\n            for i_abs in J_abs:\n                Lmask = t_abs <= i_abs\n                Rmask = t_abs > i_abs\n                \n                mL = int(np.sum(Lmask))\n                mR = int(np.sum(Rmask))\n                \n                if mL < MIN_SEG or mR < MIN_SEG:\n                    continue\n                \n                SSE_L, m_L, _, _ = fit_lstm_sse(X_all[Lmask], y_all[Lmask], epochs=epochs)\n                SSE_R, m_R, _, _ = fit_lstm_sse(X_all[Rmask], y_all[Rmask], epochs=epochs)\n                \n                T_i = log_likelihood(SSE_L, m_L) + log_likelihood(SSE_R, m_R) - LL_full\n                T_vals.append(max(0.0, T_i))\n            \n            sup_lr = max(T_vals) if T_vals else 0.0\n            \n            # Get critical value for this scale\n            crit_val = critical_values.get(k, math.inf)\n            \n            if verbose:\n                print(f\"step={l:4d} | k={k} | I_k+1=[{start_abs},{io}] | \"\n                      f\"J_k=[{J_start},{J_end}] | SupLR={sup_lr:.3f} | crit={crit_val:.3f}\")\n            \n            # Test against critical value\n            if sup_lr > crit_val:\n                if verbose:\n                    print(f\"  -> BREAK detected at step {l}, scale k={k}\")\n                selected_window_size = n_k\n                break\n            else:\n                # Accept I_k, continue to larger window\n                I_k_minus1 = Data_N[max(0, io - n_k):io]\n                n_k_minus1 = n_k\n                selected_window_size = n_k\n        \n        # Record selected window\n        results[\"windows_1\"][io - 1] = selected_window_size\n        results[\"scaled_windows_1\"][io - 1] = selected_window_size / io\n        \n        if verbose:\n            t1 = time.time()\n            print(f\"  -> Selected window: {selected_window_size} ({t1-t0:.2f}s)\")\n    \n    return pd.DataFrame(results)",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d9a46a72-e828-4dc2-88d0-bab153790c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:23.324382Z",
     "start_time": "2026-01-27T12:40:23.320428Z"
    }
   },
   "source": [
    "# Data. Generated with: https://github.com/QuantLet/AR_sim_p/tree/main\n",
    "df = pd.read_csv(\"LPA/Simulation/data.csv\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:40:31.876378Z",
     "start_time": "2026-01-27T12:40:23.332592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Run LPA Detection (Complete Pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # =========================================================================\n",
    "    # Configuration\n",
    "    # =========================================================================\n",
    "    \n",
    "    # LPA parameters\n",
    "    n_0 = 100               # Initial window size\n",
    "    c = np.sqrt(2)          # Geometric ratio\n",
    "    alpha = 0.95            # Significance level\n",
    "    \n",
    "    # Critical value computation\n",
    "    mc_reps = 100           # Monte Carlo replications\n",
    "    penalty_factor = 0.25   # Lambda for Spokoiny adjustment (0 = no adjustment)\n",
    "    \n",
    "    # Detection parameters\n",
    "    jump = 1               # Step size through time series\n",
    "    search_step = 1         # Split search granularity\n",
    "    \n",
    "    # Output directory (includes n_0 and lambda)\n",
    "    lambda_str = str(penalty_factor).replace('.', '')  # e.g., \"025\" for 0.25\n",
    "    output_dir = f\"LPA/Geometric/Jump_{jump}_N0_{n_0}/lambda{lambda_str}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Compute Critical Values\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"STEP 1: Computing Critical Values via Monte Carlo\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  n_0={n_0}, c={c:.4f}, alpha={alpha}\")\n",
    "    print(f\"  MC reps={mc_reps}, penalty_factor={penalty_factor}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    print()\n",
    "    \n",
    "    t0_cv = time.time()\n",
    "    \n",
    "    # Compute raw critical values\n",
    "    df_raw = compute_critical_values(\n",
    "        data=df[\"N\"].to_numpy(dtype=np.float32),\n",
    "        n_0=n_0,\n",
    "        c=c,\n",
    "        mc_reps=mc_reps,\n",
    "        alpha=alpha,\n",
    "        search_step=search_step,\n",
    "        max_len=len(df),\n",
    "        epochs=LSTM_EPOCHS,\n",
    "        seed=42,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Apply Spokoiny-style adjustment\n",
    "    print(f\"\\nApplying adjustment with penalty_factor={penalty_factor}...\")\n",
    "    df_adjusted = adjust_critical_values(df_raw, alpha=alpha, penalty_factor=penalty_factor)\n",
    "    \n",
    "    t1_cv = time.time()\n",
    "    print(f\"\\nCritical value computation completed in {t1_cv - t0_cv:.1f}s\")\n",
    "    \n",
    "    # Save critical values\n",
    "    raw_cv_path = f\"{output_dir}/critical_values_raw.csv\"\n",
    "    adj_cv_path = f\"{output_dir}/critical_values_adjusted.csv\"\n",
    "    df_raw.to_csv(raw_cv_path, index=False)\n",
    "    df_adjusted.to_csv(adj_cv_path, index=False)\n",
    "    print(f\"Saved: {raw_cv_path}\")\n",
    "    print(f\"Saved: {adj_cv_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    alpha_pct = int(alpha * 100)\n",
    "    print(f\"\\nCritical Values Summary (alpha={alpha}):\")\n",
    "    print(\"-\"*50)\n",
    "    for _, row in df_adjusted.iterrows():\n",
    "        print(f\"  k={int(row['k'])}: n_k={int(row['n_k'])}, \"\n",
    "              f\"CV={row[f'critical_value_{alpha_pct}']:.2f} \"\n",
    "              f\"(adj={row['adjustment_factor']:.3f})\")\n",
    "    \n",
    "    # Convert to dict for detection\n",
    "    critical_values = critical_values_to_dict(df_adjusted, alpha=alpha)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Run LPA Detection\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Running LPA-LSTM Detection\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  jump={jump}, search_step={search_step}\")\n",
    "    print()\n",
    "    \n",
    "    t0_det = time.time()\n",
    "    \n",
    "    DT_out = detect_changes_with_lstm(\n",
    "        Data_N=df[\"N\"].to_numpy(dtype=np.float32),\n",
    "        critical_values=critical_values,\n",
    "        seq_len=LSTM_SEQ_LEN,\n",
    "        n_0=n_0,\n",
    "        jump=jump,\n",
    "        search_step=search_step,\n",
    "        c=c,\n",
    "        epochs=LSTM_EPOCHS,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    t1_det = time.time()\n",
    "    print(f\"\\nDetection completed in {t1_det - t0_det:.1f}s\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Save Results\n",
    "    # =========================================================================\n",
    "    \n",
    "    out_path = f\"{output_dir}/detection_results.csv\"\n",
    "    DT_out.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved: {out_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total time: {t1_det - t0_cv:.1f}s\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Files:\")\n",
    "    print(f\"  - {raw_cv_path}\")\n",
    "    print(f\"  - {adj_cv_path}\")\n",
    "    print(f\"  - {out_path}\")"
   ],
   "id": "ba1878569d3a5a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Computing Critical Values via Monte Carlo\n",
      "======================================================================\n",
      "  n_0=100, c=1.4142, alpha=0.95\n",
      "  MC reps=100, penalty_factor=0.25\n",
      "  Device: cpu\n",
      "\n",
      "Step 1: Fitting LSTM on I_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raulbag/Documents/personal/projects/llamma/quantinar-search-exporter/.venv/XAI-Energy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I_0 size: 100\n",
      "  Residuals: 97 points, std=1.1850\n",
      "\n",
      "Step 2: Computing critical values for k=1..6\n",
      "  MC replications per scale: 100\n",
      "  Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 43\u001B[39m\n\u001B[32m     40\u001B[39m t0_cv = time.time()\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# Compute raw critical values\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m df_raw = \u001B[43mcompute_critical_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mN\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_0\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43mc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmc_reps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmc_reps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m=\u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[43msearch_step\u001B[49m\u001B[43m=\u001B[49m\u001B[43msearch_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mLSTM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     54\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[38;5;66;03m# Apply Spokoiny-style adjustment\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mApplying adjustment with penalty_factor=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpenalty_factor\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 154\u001B[39m, in \u001B[36mcompute_critical_values\u001B[39m\u001B[34m(data, n_0, c, mc_reps, alpha, search_step, max_len, seq_len, epochs, seed, verbose)\u001B[39m\n\u001B[32m    144\u001B[39m     y_sim = simulate_series_from_model(\n\u001B[32m    145\u001B[39m         model=i0_model,\n\u001B[32m    146\u001B[39m         seed_values=seed_values,\n\u001B[32m   (...)\u001B[39m\u001B[32m    150\u001B[39m         rng=rng\n\u001B[32m    151\u001B[39m     )\n\u001B[32m    153\u001B[39m     \u001B[38;5;66;03m# Compute SupLR over J_k\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     suplr = \u001B[43mcompute_suplr_for_series\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    155\u001B[39m \u001B[43m        \u001B[49m\u001B[43my_sim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msearch_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj_end\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    157\u001B[39m     suplr_values.append(suplr)\n\u001B[32m    159\u001B[39m suplr_arr = np.array(suplr_values)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mcompute_suplr_for_series\u001B[39m\u001B[34m(y, seq_len, search_step, j_start_pos, j_end_pos, epochs)\u001B[39m\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     47\u001B[39m SSE_left, m_left, _, _ = fit_lstm_sse(X_left, y_left, epochs=epochs)\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m SSE_right, m_right, _, _ = \u001B[43mfit_lstm_sse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_right\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_right\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m T_i = (log_likelihood(SSE_left, m_left) +\n\u001B[32m     51\u001B[39m        log_likelihood(SSE_right, m_right) - LL_full)\n\u001B[32m     52\u001B[39m T_vals.append(\u001B[38;5;28mmax\u001B[39m(\u001B[32m0.0\u001B[39m, T_i))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 135\u001B[39m, in \u001B[36mfit_lstm_sse\u001B[39m\u001B[34m(X, y, **kwargs)\u001B[39m\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(y) == \u001B[32m0\u001B[39m:\n\u001B[32m    133\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m math.inf, \u001B[32m0\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m model = \u001B[43mfit_lstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m yhat = predict_lstm(model, X)\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m yhat \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 91\u001B[39m, in \u001B[36mfit_lstm\u001B[39m\u001B[34m(X, y, epochs, batch_size, lr, hidden, layers, dropout)\u001B[39m\n\u001B[32m     89\u001B[39m         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n\u001B[32m     90\u001B[39m         opt.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m         \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     92\u001B[39m         opt.step()\n\u001B[32m     94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/personal/projects/llamma/quantinar-search-exporter/.venv/XAI-Energy/lib/python3.12/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/personal/projects/llamma/quantinar-search-exporter/.venv/XAI-Energy/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/personal/projects/llamma/quantinar-search-exporter/.venv/XAI-Energy/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed59f59c9fe31c39"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
